from torchvision.transforms import InterpolationMode

from models.vae.autoencoder import AutoencoderKL
import torch
import config.configVAE as cfg
import torch.nn.functional as F
import matplotlib.pyplot as plt
import os
from datetime import datetime
from torch.utils.data import DataLoader
from dataloader.dataLoadvae import MarioDataset
from train import setup_logging, save_loss_curve


device: str = "cuda" if torch.cuda.is_available() else "cpu"

def save_model_with_optimizer(model, optimizer, epochs, final_loss, best_loss, loss_history, path=cfg.ckpt_path):
    """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹åˆ°ckptç›®å½•ï¼ŒåŒ…å«ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨çŠ¶æ€"""
    if not os.path.exists(path):
        os.makedirs(path)

    # ç”Ÿæˆæ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³å’Œepochä¿¡æ¯ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H")
    model_filename = f"vae_epoch{epochs}_{timestamp}.pth"
    model_path = os.path.join(path, model_filename)

    # å‡†å¤‡ä¿å­˜çš„æ•°æ®
    save_data = {
        'network_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'epoch': epochs,
        'loss': final_loss,
        'best_loss': best_loss,
        'loss_history': loss_history,
        'model_name': 'SDVAE',
        'batch_size': cfg.batch_size,
    }

    # ä¿å­˜æ¨¡å‹
    try:
        torch.save(save_data, model_path)
        print(f"âœ… VAE model saved to {model_path}")

    except Exception as e:
        print(f"âŒ Save VAE model failed: {e}")

def infer_test(img):
    device_obj = torch.device(device)
    model = AutoencoderKL()
    
    # åªæœ‰å½“ model_path ä¸ä¸ºç©ºæ—¶æ‰æ‹¼æ¥è·¯å¾„
    if cfg.model_path:
        ckpt_path = os.path.join(cfg.ckpt_path, cfg.model_path)
    else:
        ckpt_path = cfg.ckpt_path
    
    if os.path.exists(ckpt_path):
        print(f"ğŸ“¥ load pretrained checkpoint: {ckpt_path}")
        checkpoint = torch.load(ckpt_path, map_location=device_obj, weights_only=False)
        
        # å¤„ç† torch.compile å¯¼è‡´çš„ _orig_mod. å‰ç¼€é—®é¢˜
        state_dict = checkpoint['network_state_dict']
        # æ£€æŸ¥æ˜¯å¦æœ‰ _orig_mod. å‰ç¼€
        if any(key.startswith('_orig_mod.') for key in state_dict.keys()):
            print("ğŸ”§ Detected _orig_mod. prefix in checkpoint (from torch.compile), removing...")
            # åˆ›å»ºæ–°çš„ state_dictï¼Œå»æ‰ _orig_mod. å‰ç¼€
            new_state_dict = {}
            for key, value in state_dict.items():
                if key.startswith('_orig_mod.'):
                    new_key = key[len('_orig_mod.'):]  # å»æ‰ _orig_mod. å‰ç¼€
                    new_state_dict[new_key] = value
                else:
                    new_state_dict[key] = value
            state_dict = new_state_dict
        
        model.load_state_dict(state_dict, strict=False)
        print("ckpt loaded successfully")
    else:
        print(f"âš ï¸ Checkpoint not found: {ckpt_path}, use initialized model")
    
    model = model.to(device_obj)
    vae_test(img, model, device_obj, 9999)





def vae_test(img_path, model, device_obj, e=99999, out_dir='output/VAE', logger=None ):
    """æµ‹è¯•VAEæ¨¡å‹çš„ç¼–ç è§£ç æ•ˆæœ"""
    import os
    import glob
    import time
    from PIL import Image
    import torchvision.transforms as transforms
    import torch.nn.functional as F

    if not os.path.exists(out_dir):
        os.makedirs(out_dir)
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = out_dir+f"/epoch{e+1}"
    os.makedirs(output_dir, exist_ok=True)
    
    # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
    if os.path.isfile(img_path):
        img_files = [img_path]
    else:
        img_files = glob.glob(os.path.join(img_path, "*.png")) + glob.glob(os.path.join(img_path, "*.jpg"))
    
    if not img_files:
        print(f"âŒ No images found in {img_path}")
        return
    
    model.eval()
    total_loss = 0
    num_images = 0
    
    # æ—¶é—´ç»Ÿè®¡å˜é‡
    total_encode_time = 0.0
    total_decode_time = 0.0
    total_process_time = 0.0
    
    # å®šä¹‰å›¾åƒå˜æ¢
    transform = transforms.Compose([
        transforms.Resize((128, 128),interpolation=InterpolationMode.NEAREST),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])
    
    with torch.no_grad():
        for img_file in img_files[:10]:  # é™åˆ¶æµ‹è¯•å›¾ç‰‡æ•°é‡
            try:
                # åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
                img = Image.open(img_file).convert('RGB')
                img_tensor = transform(img).unsqueeze(0).to(device_obj)
                
                # åŒæ­¥CUDAæ“ä½œä»¥ç¡®ä¿å‡†ç¡®è®¡æ—¶
                if device_obj.type == 'cuda':
                    torch.cuda.synchronize()
                
                # è®¡æ—¶ï¼šVAEç¼–ç 
                encode_start = time.time()
                encoded = model.encode(img_tensor)
                latent = encoded.sample()
                if device_obj.type == 'cuda':
                    torch.cuda.synchronize()
                encode_end = time.time()
                encode_time = encode_end - encode_start
                total_encode_time += encode_time
                
                # è®¡æ—¶ï¼šVAEè§£ç 
                decode_start = time.time()
                decoded = model.decode(latent)
                if device_obj.type == 'cuda':
                    torch.cuda.synchronize()
                decode_end = time.time()
                decode_time = decode_end - decode_start
                total_decode_time += decode_time
                
                total_process_time += (encode_time + decode_time)
                
                
                # ä¿å­˜åŸå§‹å›¾åƒå’Œé‡å»ºå›¾åƒ
                img_name = os.path.splitext(os.path.basename(img_file))[0]
                
                # è½¬æ¢ä¸ºå¯ä¿å­˜çš„æ ¼å¼
                original_img = (img_tensor[0].cpu() * 0.5 + 0.5).clamp(0, 1)
                reconstructed_img = (decoded[0].cpu() * 0.5 + 0.5).clamp(0, 1)
                
                # ä¿å­˜å›¾åƒ
                transforms.ToPILImage()(original_img).save(os.path.join(output_dir, f"{img_name}_original.png"))
                transforms.ToPILImage()(reconstructed_img).save(os.path.join(output_dir, f"{img_name}_reconstructed.png"))
                
                num_images += 1
                
            except Exception as ex:
                print(f"âŒ Error processing {img_file}: {ex}")
                continue
    
    # è®¡ç®—å¹³å‡æ—¶é—´
    if num_images > 0:
        avg_encode_time = total_encode_time / num_images
        avg_decode_time = total_decode_time / num_images
        avg_total_time = total_process_time / num_images
        
        # è®°å½•æ—¶é—´ç»Ÿè®¡ä¿¡æ¯
        time_message = f"â±ï¸  VAE Performance (Epoch {e+1}): Avg Encode: {avg_encode_time*1000:.2f}ms, " \
                      f"Avg Decode: {avg_decode_time*1000:.2f}ms, Avg Total: {avg_total_time*1000:.2f}ms " \
                      f"(processed {num_images} images)"
        if logger is not None:
            logger.info(time_message)
    
    model.train()  # æ¢å¤è®­ç»ƒæ¨¡å¼


def train():
    logger, log_path = setup_logging()
    device_obj = torch.device(device)
    model = AutoencoderKL().to(device_obj)

    epochs = cfg.epochs
    loss_log_iter = cfg.loss_log_iter
    img_save_epoch = cfg.img_save_epoch
    batch_size = cfg.batch_size
    ckpt_save_epoch = cfg.checkpoint_save_epoch
    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=1e-5)

    dataset = MarioDataset(cfg)
    dataloader = DataLoader(dataset, batch_size=batch_size,shuffle=True,num_workers=cfg.num_workers)

    # ä½¿ç”¨å…¨éƒ¨æ•°æ®è¿›è¡Œè®­ç»ƒ
    total_samples = len(dataset)
    print(f"ğŸ“Š Using all {total_samples} samples for training")

    # æ£€æŸ¥æ˜¯å¦æœ‰é¢„è®­ç»ƒæ£€æŸ¥ç‚¹
    start_epoch = 0
    final_avg_loss = 0  # ç”¨äºä¿å­˜æœ€ç»ˆçš„avg_loss
    best_loss = float('inf')
    loss_history = []
    if cfg.resume_training:
        # æŒ‰æ–‡ä»¶åæ’åºï¼Œè·å–æœ€æ–°çš„æ£€æŸ¥ç‚¹
        latest_checkpoint = cfg.resume_checkpoint_path
        try:
            print(f"ğŸ“¥ Loading checkpoint: {latest_checkpoint}")
            checkpoint = torch.load(latest_checkpoint, map_location=device_obj, weights_only=False)
            
            # å¤„ç† torch.compile å¯¼è‡´çš„ _orig_mod. å‰ç¼€é—®é¢˜
            state_dict = checkpoint['network_state_dict']
            # æ£€æŸ¥æ˜¯å¦æœ‰ _orig_mod. å‰ç¼€
            if any(key.startswith('_orig_mod.') for key in state_dict.keys()):
                print("ğŸ”§ Detected _orig_mod. prefix in checkpoint (from torch.compile), removing...")
                # åˆ›å»ºæ–°çš„ state_dictï¼Œå»æ‰ _orig_mod. å‰ç¼€
                new_state_dict = {}
                for key, value in state_dict.items():
                    if key.startswith('_orig_mod.'):
                        new_key = key[len('_orig_mod.'):]  # å»æ‰ _orig_mod. å‰ç¼€
                        new_state_dict[new_key] = value
                    else:
                        new_state_dict[key] = value
                state_dict = new_state_dict
            
            model.load_state_dict(state_dict, strict=False)
            opt.load_state_dict(checkpoint.get('optimizer_state_dict', {}))
            start_epoch = checkpoint.get('epoch', 0)
            best_loss = checkpoint.get('best_loss', float('inf'))
            loss_history = checkpoint.get('loss_history', [])
            
            print(f"âœ… Checkpoint loaded successfully! Starting from epoch {start_epoch + 1}")
            print(f"ğŸ“Š Previous best loss: {best_loss:.6f}")
            
        except Exception as e:
            print(f"âŒ Failed to load checkpoint: {e}")
            print("ğŸ”„ Starting training from scratch...")

    # ä½¿ç”¨torch.compileåŠ é€Ÿè®­ç»ƒï¼ˆéœ€è¦PyTorch 2.0+ï¼‰
    if cfg.use_torch_compile:
        try:
            # æ£€æŸ¥PyTorchç‰ˆæœ¬
            if hasattr(torch, 'compile'):
                print("ğŸš€ Compiling model with torch.compile for faster training...")
                model = torch.compile(model, mode='max-autotune')  # modeå¯é€‰: 'default', 'reduce-overhead', 'max-autotune'
                print("âœ… Model compiled successfully! (Note: First training batch will be slower due to compilation)")
            else:
                print("âš ï¸  torch.compile not available (requires PyTorch 2.0+), skipping compilation")
        except Exception as e:
            print(f"âš ï¸  Failed to compile model: {e}, continuing without compilation")
    
    model.train()
    for e in range(start_epoch, epochs):
        total_loss = 0
        batch_count = 0
        
        for batch_img in dataloader:
            # ä½¿ç”¨æ‰“ä¹±åçš„ç´¢å¼•è·å–æ•°æ®
            try:
                # VAEå‰å‘ä¼ æ’­
                batch_img = batch_img.to(device_obj)
                encoded = model.encode(batch_img)
                latent = encoded.sample()
                decode_img = model.decode(latent)
                
                # åªä½¿ç”¨L1é‡å»ºæŸå¤±
                loss = F.l1_loss(decode_img, batch_img)
                
                opt.zero_grad()
                loss.backward()
                opt.step()
                total_loss += loss.item()
                batch_count += 1
            except Exception as e:
                print(f"   âŒ error in training step: {e}")
                print(f"    batch_data shapes: {batch_img.shape}")
                raise e

            if batch_count % loss_log_iter ==0:
                batch_loss = loss.item()
                loss_message = f"Epoch {e + 1}/{epochs}, in batch: {batch_count},  Loss: {batch_loss:.6f}"
                logger.info(loss_message)

        # ä¸€ä¸ªepoch
        if batch_count > 0:
            avg_loss = total_loss / batch_count
            final_avg_loss = avg_loss  # æ›´æ–°æœ€ç»ˆçš„avg_loss
            
            # æ¯ 1 ä¸ªepochæ‰“å°ä¸€æ¬¡æŸå¤±å¹¶è®°å½•åˆ°å†å²
            loss_history.append(avg_loss)  # åªè®°å½•æ‰“å°çš„æŸå¤±å€¼
            loss_message = f"Epoch {e + 1}/{epochs}, Train Loss: {avg_loss:.6f}"
            logger.info(loss_message)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºè®­ç»ƒæŸå¤±ï¼‰
            is_best = avg_loss < best_loss
            if is_best:
                # ç«‹å³æ›´æ–°æœ€ä½³æŸå¤±
                improvement = (best_loss - avg_loss) / best_loss if best_loss != float('inf') else 1.0
                best_loss = avg_loss
                best_message = f"This is the new best training loss(improvement: {improvement:.2%})"
                logger.info(best_message)

        if (e + 1) % img_save_epoch == 0:
            vae_test(cfg.test_img_path,model,device_obj,e,cfg.out_dir,logger)

        if (e + 1) % ckpt_save_epoch == 0:
            current_loss = avg_loss if batch_count > 0 else 0
            save_model_with_optimizer(model, opt, e + 1, current_loss, best_loss, loss_history, path=cfg.ckpt_path)
            checkpoint_message = f"ğŸ’¾ Checkpoint saved at epoch {e + 1}"
            logger.info(checkpoint_message)

    completion_message = "Training completed!"
    print(completion_message)
    logger.info(completion_message)
    if epochs >= 1 and final_avg_loss > 0:
        save_message = "ğŸ’¾ save final training model..."
        print(save_message)
        logger.info(save_message)

        save_model_with_optimizer(model, opt, epochs, final_avg_loss, best_loss, loss_history, path=cfg.ckpt_path)

        # è®°å½•è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        stats_message = f"ğŸ“Š training statistics: total epochs: {epochs}, best loss: {best_loss:.6f}, final loss: {final_avg_loss:.6f}, batches per epoch: {batch_count}"
        print(f"ğŸ“Š training statistics:")
        print(f"    total epochs: {epochs}")
        print(f"    best loss: {best_loss:.6f}")
        print(f"    final loss: {final_avg_loss:.6f}")
        print(f"    batches per epoch: {batch_count}")
        logger.info(stats_message)

        vae_test(cfg.test_img_path,model,device_obj,9999,cfg.out_dir,logger)

        if len(loss_history) > 0:
            final_loss_curve_path = save_loss_curve(loss_history, 1, save_path=cfg.out_dir)
            logger.info(f"Final loss curve saved to: {final_loss_curve_path}")

        # è®°å½•æ—¥å¿—æ–‡ä»¶è·¯å¾„
        final_log_message = f"log path: {log_path}"
        print(final_log_message)
        logger.info(final_log_message)

def arg():
    import argparse
    parser = argparse.ArgumentParser('vae train')
    parser.add_argument('-tr',"--train", type = str)
    parser.add_argument('-in',"--infer",type = str)
    parser.add_argument('-i',"--img",type = str, default="eval_data/vae")
    return parser.parse_args()

if __name__ == "__main__":
    args = arg()
    if args.train == "tr":
        print(" train...")
        train()
    elif args.infer == "in":
        print(" infer..")
        infer_test(args.img)
    else:
        print(" train...")
        train()